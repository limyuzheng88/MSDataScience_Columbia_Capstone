{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV1Hzm1UTFZ2",
        "outputId": "09cf4cbc-be69-46f2-e08d-77670b47906f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swuUJYZDS3OA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "def parse_output(output):\n",
        "  pattern = r\"Intermediate results for Run \\d+: (\\{.*?\\})(?=\\n|$)\"\n",
        "  matches = re.findall(pattern, output, re.DOTALL)\n",
        "\n",
        "  parsed_results = []\n",
        "  for match in matches:\n",
        "      # Remove newline characters and excess whitespace in array definitions\n",
        "      cleaned_match = re.sub(r\"\\n\\s*\", \" \", match)\n",
        "      # Replace `array([...])` with list format\n",
        "      cleaned_match = re.sub(r\"array\\((\\[.*?\\])\\)\", r\"\\1\", cleaned_match)\n",
        "\n",
        "      try:\n",
        "          # Parse using ast.literal_eval\n",
        "          parsed_results.append(ast.literal_eval(cleaned_match))\n",
        "      except ValueError as e:\n",
        "          print(cleaned_match)\n",
        "          print(f\"Could not parse: {match}\")\n",
        "          print(f\"Error: {e}\")\n",
        "\n",
        "  # Convert to DataFrame\n",
        "  df = pd.DataFrame(parsed_results)\n",
        "  return df\n",
        "# print(annony_df)\n",
        "\n",
        "# Convert to DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    average_precision_score, roc_auc_score\n",
        ")\n",
        "import ast\n",
        "\n",
        "# res is the output fron the above function\n",
        "def calculate_metrics(res, true_labels):\n",
        "# Assuming test['Approved_Flag'] has the true labels\n",
        "# true_labels = test_ohe_100[\"Approved_Flag\"].tolist()\n",
        "  experiments = res.copy()\n",
        "# Initialize lists to store metrics for each row\n",
        "  accuracies = []\n",
        "  precisions = []\n",
        "  recalls = []\n",
        "  f1_scores = []\n",
        "  pr_aucs = []\n",
        "  roc_aucs = []\n",
        "\n",
        "  # Iterate over each row in experiments\n",
        "  for _, row in experiments.iterrows():\n",
        "      predictions = row['Prediction']\n",
        "\n",
        "      # Check if predictions is a string; if so, convert to list\n",
        "      if isinstance(predictions, str):\n",
        "          try:\n",
        "              predictions = ast.literal_eval(predictions)\n",
        "          except ValueError as e:\n",
        "              print(f\"Error converting string to list in row {_}: {e}\")\n",
        "              predictions = []  # Set to empty list or handle appropriately\n",
        "\n",
        "      # Check if predictions contain \"invalid\"\n",
        "      if \"invalid\" in str(predictions).lower():\n",
        "          # Set all metrics to \"invalid\" if the prediction is invalid\n",
        "          accuracy = precision = recall = f1 = pr_auc = roc_auc = \"invalid\"\n",
        "      elif isinstance(predictions, list) and len(predictions) == len(true_labels):\n",
        "          # Convert predictions to binary labels (0 or 1) if needed for other metrics\n",
        "          binary_predictions = [1 if p >= 0.5 else 0 for p in predictions]\n",
        "\n",
        "          # Basic metrics\n",
        "          accuracy = accuracy_score(true_labels, binary_predictions)\n",
        "          precision = precision_score(true_labels, binary_predictions, zero_division=1)\n",
        "          recall = recall_score(true_labels, binary_predictions, zero_division=1)\n",
        "          f1 = f1_score(true_labels, binary_predictions, zero_division=1)\n",
        "\n",
        "          # AUC metrics\n",
        "          pr_auc = average_precision_score(true_labels, predictions)\n",
        "          roc_auc = roc_auc_score(true_labels, predictions)\n",
        "      else:\n",
        "          # Set metrics to None if predictions are invalid\n",
        "          accuracy = precision = recall = f1 = pr_auc = roc_auc = None\n",
        "\n",
        "      # Append to lists\n",
        "      accuracies.append(accuracy)\n",
        "      precisions.append(precision)\n",
        "      recalls.append(recall)\n",
        "      f1_scores.append(f1)\n",
        "      pr_aucs.append(pr_auc)\n",
        "      roc_aucs.append(roc_auc)\n",
        "\n",
        "  # Add metrics to experiments DataFrame\n",
        "  experiments['Accuracy'] = accuracies\n",
        "  experiments['Precision'] = precisions\n",
        "  experiments['Recall'] = recalls\n",
        "  experiments['F1_Score'] = f1_scores\n",
        "  experiments['PR_AUC'] = pr_aucs\n",
        "  experiments['ROC_AUC'] = roc_aucs\n",
        "\n",
        "  return experiments\n"
      ],
      "metadata": {
        "id": "HGdxuCKNS_nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example usage"
      ],
      "metadata": {
        "id": "3p2vPVWa8V_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = \"\"\"\n",
        "Evaluating Set ID: Set_1_Prop_0.1\n",
        "Num of non-numeric or invalid: 0\n",
        "Intermediate results for Run 1: {'Num Features': 10, 'Sample Size': 64, 'Class 1 Proportion': 0.1, 'Set ID': 'Set_1_Prop_0.1', 'Run Number': 1, 'Accuracy': 0.87, 'Precision': 0.0, 'Recall': 0.0, 'F1 Score': 0.0, 'Prediction': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "\n",
        "Evaluating Set ID: Set_2_Prop_0.1\n",
        "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
        "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
        "Num of non-numeric or invalid: 0\n",
        "Intermediate results for Run 1: {'Num Features': 10, 'Sample Size': 64, 'Class 1 Proportion': 0.1, 'Set ID': 'Set_2_Prop_0.1', 'Run Number': 1, 'Accuracy': 0.9, 'Precision': 1.0, 'Recall': 0.23076923076923078, 'F1 Score': 0.375, 'Prediction': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "\n",
        "Evaluating Set ID: Set_3_Prop_0.1\n",
        "Num of non-numeric or invalid: 0\n",
        "Intermediate results for Run 1: {'Num Features': 10, 'Sample Size': 64, 'Class 1 Proportion': 0.1, 'Set ID': 'Set_3_Prop_0.1', 'Run Number': 1, 'Accuracy': 0.87, 'Precision': 0.0, 'Recall': 0.0, 'F1 Score': 0.0, 'Prediction': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sUVytCZEUFdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = parse_output(output)"
      ],
      "metadata": {
        "id": "xfDqb5qESx6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.to_csv(\"experiments_result_t_table_show_7B-GTL-8bit_48\")"
      ],
      "metadata": {
        "id": "lX2DD9KFS472"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}