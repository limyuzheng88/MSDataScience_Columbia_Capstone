{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f49dbe67e8d34286b34b6072b5dd82ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_38bbfd9eae10488abaf17000e232beec"
          }
        },
        "2f724cce11164ba2b904841583727937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f1151e8323c4dc09e90b104d6babccc",
            "placeholder": "​",
            "style": "IPY_MODEL_301b96453321434e83b81706f71d99ae",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e1b025b097264d40ac3fda8dfb4a7e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_26e8eb7958ee48e1900aeece03b06811",
            "placeholder": "​",
            "style": "IPY_MODEL_ceffdf1d10d4487998e0786a95229f24",
            "value": ""
          }
        },
        "75c865a996364bd0a3234b91f19cac28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b12e9b3de1f843a4a64c9207b348f930",
            "style": "IPY_MODEL_bc71d08cfa5847a09f1edef36c43bb76",
            "value": true
          }
        },
        "aece8b248fdf4366bf5f794317ed83cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d88626ee8a95412ab85448e3b9cf39d7",
            "style": "IPY_MODEL_52be277d06f94de295094b1bf0f6f478",
            "tooltip": ""
          }
        },
        "0ff2cba99cfd4ddcafe0627f1acbea76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_591fbecae1924fe38884057520fc9b28",
            "placeholder": "​",
            "style": "IPY_MODEL_dd2c3c68de1d4e18b7a7765875ced611",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "38bbfd9eae10488abaf17000e232beec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "3f1151e8323c4dc09e90b104d6babccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301b96453321434e83b81706f71d99ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26e8eb7958ee48e1900aeece03b06811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceffdf1d10d4487998e0786a95229f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b12e9b3de1f843a4a64c9207b348f930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc71d08cfa5847a09f1edef36c43bb76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d88626ee8a95412ab85448e3b9cf39d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52be277d06f94de295094b1bf0f6f478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "591fbecae1924fe38884057520fc9b28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd2c3c68de1d4e18b7a7765875ced611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddcbdd0de3814ed6a16287d06d27c98c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_770e060046a642b787adcd7ddda7f16a",
            "placeholder": "​",
            "style": "IPY_MODEL_1ca89751a04a439283d32030eae09c67",
            "value": "Connecting..."
          }
        },
        "770e060046a642b787adcd7ddda7f16a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ca89751a04a439283d32030eae09c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38c66ebc30a94fc693d42989b2a7aa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd63b64b39084fdebfb3de735f3d2b38",
              "IPY_MODEL_c34ed5b564e9465c9d6ba39ffb19aef5",
              "IPY_MODEL_611370cbaecc40b98e88d942349d39ac"
            ],
            "layout": "IPY_MODEL_e1edaf0164d7481294da7d76a7bdd9db"
          }
        },
        "bd63b64b39084fdebfb3de735f3d2b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c09bb8e6e90471892115e25388bf9b1",
            "placeholder": "​",
            "style": "IPY_MODEL_ccf070ea1792405e85bceb87867a69a1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c34ed5b564e9465c9d6ba39ffb19aef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea752142dc446fe8f98c6a4378ec3f0",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_218181febe794ee78ec6c1f8c32239fe",
            "value": 6
          }
        },
        "611370cbaecc40b98e88d942349d39ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b891f72e73474881b3b9ffa56cbfdd7a",
            "placeholder": "​",
            "style": "IPY_MODEL_f2be64f79f60489db94aaa00857f70da",
            "value": " 6/6 [01:19&lt;00:00, 12.13s/it]"
          }
        },
        "e1edaf0164d7481294da7d76a7bdd9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c09bb8e6e90471892115e25388bf9b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccf070ea1792405e85bceb87867a69a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cea752142dc446fe8f98c6a4378ec3f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "218181febe794ee78ec6c1f8c32239fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b891f72e73474881b3b9ffa56cbfdd7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2be64f79f60489db94aaa00857f70da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1TR0svRc6kSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc7b159-1bf3-494c-ffe7-6192bb92e098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "!!pip install hf_transfer"
      ],
      "metadata": {
        "id": "6U9g7RqXzTBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43e6267-407d-4cab-9de8-d96e0b7370f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting hf_transfer',\n",
              " '  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)',\n",
              " 'Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/3.6 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.1/3.6 MB\\x1b[0m \\x1b[31m3.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.1/3.6 MB\\x1b[0m \\x1b[31m16.6 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m3.6/3.6 MB\\x1b[0m \\x1b[31m39.6 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m3.6/3.6 MB\\x1b[0m \\x1b[31m32.1 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hInstalling collected packages: hf_transfer',\n",
              " 'Successfully installed hf_transfer-0.1.8']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "feature_imp_ohe = pd.read_csv('/content/drive/MyDrive/Indian_bank_data/rf_importances.csv')\n",
        "feature_imp_label = pd.read_csv('/content/drive/MyDrive/Indian_bank_data/rf_importances_label.csv')\n",
        "train_ohe = pd.read_csv('/content/drive/MyDrive/Indian_bank_data/train_tree.csv')\n",
        "train_ohe = train_ohe[~train_ohe[\"Approved_Flag\"].isna()]\n",
        "train_label = pd.read_csv('/content/drive/MyDrive/Indian_bank_data/train_label.csv')\n",
        "train_label = train_label[~train_label[\"Approved_Flag\"].isna()]\n",
        "test_ohe = pd.read_csv('/content/drive/MyDrive/Indian_bank_data/test_tree.csv')\n",
        "test_label = pd.read_csv('/content/drive/MyDrive/Indian_bank_data/test_label.csv')\n",
        "test_ohe = test_ohe[~test_ohe[\"Approved_Flag\"].isna()]\n",
        "test_label = test_label[~test_label[\"Approved_Flag\"].isna()]\n",
        "test_ohe_100 = test_ohe.sample(n=100, random_state=42)\n",
        "test_label_100 = test_label.sample(n=100, random_state=42)"
      ],
      "metadata": {
        "id": "LKOWPTvVjhjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_description = pd.read_csv('/content/drive/MyDrive/Indian_bank_data/Capstone - Data Dictionary - Sheet1.csv')"
      ],
      "metadata": {
        "id": "5XtbjuyqxLke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_last_column_in_last_n_rows(df, n, sample_size=32):\n",
        "    sampled_df = df.sample(n=sample_size, random_state=42) if len(df) >= sample_size else df.copy()\n",
        "\n",
        "    formatted_text = \"--- data ---\\n\"\n",
        "    formatted_text += \"|\" + \"|\".join(sampled_df.columns) + \"|\\n\"  # Header row with column names\n",
        "\n",
        "    # List to store the actual masked values for the last n rows\n",
        "    masked_values = []\n",
        "\n",
        "    # Iterate over rows and apply masking for the last n rows\n",
        "    for i, (_, row) in enumerate(sampled_df.iterrows(), start=1):\n",
        "        row_values = list(row.values)\n",
        "        if i > len(sampled_df) - n:  # Apply mask to the last n rows\n",
        "            if n == 1:\n",
        "                row_values[-1] = \"<MASK>\"\n",
        "            else:\n",
        "                mask_num = i - (len(sampled_df) - n)\n",
        "                row_values[-1] = f\"<MASK{mask_num}>\"\n",
        "            masked_values.append(row[row.index[-1]])  # Store the actual value being masked\n",
        "        formatted_text += \"|\" + \"|\".join(str(value) for value in row_values) + \"|\\n\"\n",
        "\n",
        "    if n == 1:\n",
        "        formatted_text += \"\\nPlease use the supplied data to predict the <MASK> Approved_Flag.\\n\"\n",
        "    else:\n",
        "        formatted_text += f\"\\nPlease use the supplied data to predict the <MASK{k}> Approved_Flag for k = 1,2,...,{n}.\\n\"\n",
        "\n",
        "    # Add actual masked values separated by a comma\n",
        "    formatted_text += \"Answer: \" + \", \".join(str(val) for val in masked_values)\n",
        "\n",
        "    return formatted_text\n"
      ],
      "metadata": {
        "id": "H2uHiE6KytGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_part = \"\"\"You are an expert in the financial sector and banking industry.\n",
        "Based on the credit information of individuals, please predict the Approval_Flag.\n",
        "I will supply multiple instances with features and the corresponding label for your reference.\n",
        "Please refer to the table below for detailed descriptions of the features and label:\\n\"\"\""
      ],
      "metadata": {
        "id": "6T1zbWRAp5IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_feature_label_descriptions(data_dict_df):\n",
        "\n",
        "    feature_descriptions = \"--- feature description ---\\n\"\n",
        "    label_descriptions = \"--- label description ---\\n\"\n",
        "\n",
        "    # Process all rows except the last one as feature descriptions\n",
        "    for _, row in data_dict_df.iloc[:-1].iterrows():\n",
        "        feature_name = row['Feature']\n",
        "        description = row['Feature Description']\n",
        "        feature_descriptions += f\"{feature_name}: {description}\\n\"\n",
        "\n",
        "    # Treat the last row as the label description\n",
        "    label_row = data_dict_df.iloc[-1]\n",
        "    label_name = label_row['Feature']\n",
        "    label_description = label_row['Feature Description']\n",
        "    label_descriptions += f\"{label_name}: {label_description}\\n\"\n",
        "\n",
        "    # Combine both sections\n",
        "    return feature_descriptions + label_descriptions\n",
        "\n"
      ],
      "metadata": {
        "id": "-PzQjsbaolB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# description_prompt = generate_feature_label_descriptions(feature_description)"
      ],
      "metadata": {
        "id": "f_-jVC_exaUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()  # Required for private or licensed models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "f49dbe67e8d34286b34b6072b5dd82ff",
            "2f724cce11164ba2b904841583727937",
            "e1b025b097264d40ac3fda8dfb4a7e29",
            "75c865a996364bd0a3234b91f19cac28",
            "aece8b248fdf4366bf5f794317ed83cd",
            "0ff2cba99cfd4ddcafe0627f1acbea76",
            "38bbfd9eae10488abaf17000e232beec",
            "3f1151e8323c4dc09e90b104d6babccc",
            "301b96453321434e83b81706f71d99ae",
            "26e8eb7958ee48e1900aeece03b06811",
            "ceffdf1d10d4487998e0786a95229f24",
            "b12e9b3de1f843a4a64c9207b348f930",
            "bc71d08cfa5847a09f1edef36c43bb76",
            "d88626ee8a95412ab85448e3b9cf39d7",
            "52be277d06f94de295094b1bf0f6f478",
            "591fbecae1924fe38884057520fc9b28",
            "dd2c3c68de1d4e18b7a7765875ced611",
            "ddcbdd0de3814ed6a16287d06d27c98c",
            "770e060046a642b787adcd7ddda7f16a",
            "1ca89751a04a439283d32030eae09c67"
          ]
        },
        "id": "LFq_keI-X_TL",
        "outputId": "4d63f393-dc1d-45fb-8df9-a89af29774e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f49dbe67e8d34286b34b6072b5dd82ff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to load the model**\n",
        "\n",
        "\n",
        "1.   Regster for LLaMA on Meta website\n",
        "2.   Download the model data using Hugging Face or Github (If you use hugginface, your directory is like the following: /content/LLaMA-2-7b/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9. The 01c7... includes config.json)\n",
        "3.   Download files in manage_ckpt in the Model github (or you can fork the github repo) and run the following LLAMA_MODEL_PATH=\"/content/LLaMA-2-7b/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9\"\n",
        "CKPT_SAVE_DIR=\"/content\"                # Directory to save the final model\n",
        "MODEL_SIZE=\"7B\"                         # Specify \"7B\" or \"13B\"\n",
        "\n",
        "  !bash /content/recover_model.sh {dollarmark}LLAMA_MODEL_PATH {dollarmark}CKPT_SAVE_DIR {dollarmark}MODEL_SIZE, {dollarmark} = $\n",
        "\n",
        "4.    Load model by the following recovered_model_path = \"/content/LLaMA-2-GTL/7B\"\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(recovered_model_path)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(recovered_model_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "tV66RmpP5Vaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is to download meta's original llama model. Skip, since we already have downloaded it once"
      ],
      "metadata": {
        "id": "hSrxFa7H66zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recovered_model_path = \"/content/drive/MyDrive/LLaMA-2-7b-checkpoint-GTL\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(recovered_model_path)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(recovered_model_path)\n",
        "\n",
        "# base_model_path = \"/content/drive/MyDrive/LLaMA-2-13b\"  # Define a path to save the model\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-hf\", cache_dir=base_model_path)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\", cache_dir=base_model_path)"
      ],
      "metadata": {
        "id": "wHWJURCcT7eY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "38c66ebc30a94fc693d42989b2a7aa65",
            "bd63b64b39084fdebfb3de735f3d2b38",
            "c34ed5b564e9465c9d6ba39ffb19aef5",
            "611370cbaecc40b98e88d942349d39ac",
            "e1edaf0164d7481294da7d76a7bdd9db",
            "8c09bb8e6e90471892115e25388bf9b1",
            "ccf070ea1792405e85bceb87867a69a1",
            "cea752142dc446fe8f98c6a4378ec3f0",
            "218181febe794ee78ec6c1f8c32239fe",
            "b891f72e73474881b3b9ffa56cbfdd7a",
            "f2be64f79f60489db94aaa00857f70da"
          ]
        },
        "outputId": "c13cc4a7-3377-4396-fc39-cbb0b495c473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38c66ebc30a94fc693d42989b2a7aa65"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is to download the GTL-enhanced model weight differences and recover the LLaMA-2-GTL checkpoint. Skip, since we already have downloaded it once"
      ],
      "metadata": {
        "id": "PFACB0B17EP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLAMA_MODEL_PATH=\"/content/drive/MyDrive/LLaMA-2-7b/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9\"\n",
        "# CKPT_SAVE_DIR=\"/content/drive/MyDrive/LLaMA-2-7b-checkpoint\"                # Directory to save the final model\n",
        "# MODEL_SIZE=\"7B\"                         # Specify \"7B\" or \"13B\"\n",
        "\n",
        "# LLAMA_MODEL_PATH=\"/content/drive/MyDrive/LLaMA-2-13b/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1\"\n",
        "# CKPT_SAVE_DIR=\"/content/drive//MyDrive/LLaMA-2-13b-checkpoint\"                # Directory to save the final model\n",
        "# MODEL_SIZE=\"13B\"                         # Specify \"7B\" or \"13B\"\n",
        "\n",
        "# Run the recovery script\n",
        "# !bash /content/drive/MyDrive/Industrial-Foundation-Models/scripts/manage_ckpt/recover_model.sh $LLAMA_MODEL_PATH $CKPT_SAVE_DIR $MODEL_SIZE"
      ],
      "metadata": {
        "id": "k6ytpys2YMC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need this if u running quantized models\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "# # after pip install, need to restart colab session (not \"Disconnect n delete runtime\"), in order to load_in_4bit for next cell's recovery of model"
      ],
      "metadata": {
        "id": "pw_a8j5u716A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# # Load the recovered model\n",
        "recovered_model_path = \"/content/drive/MyDrive/LLaMA-2-7b-checkpoint-GTL-Delta\"\n",
        "# recovered_model_path = \"/content/drive/MyDrive/LLaMA-2-13b-checkpoint-GTL-Delta\"\n",
        "\n",
        "# # remove load_in_4bit=True if running original unquantized model\n",
        "model = AutoModelForCausalLM.from_pretrained(recovered_model_path, load_in_8bit=True) #, load_in_4bit=True, device_map=\"auto\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(recovered_model_path) #, device_map=\"auto\""
      ],
      "metadata": {
        "id": "_PC8-7q4YRjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Q4q_6Q5L0c",
        "outputId": "dd7157ee-8e44-4924-d489-1532b119b195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Function to create multiple sets of in-context example: input should be size of sample and number of set, with the dataframe, return with id of the set\n",
        "\n",
        "def create_in_context_example_sets(df, sample_size, num_sets, target_column, proportions):\n",
        "    example_sets = {}\n",
        "    set_counter = 1\n",
        "\n",
        "    # Generate sets for each specified proportion of class 1\n",
        "    for prop_1 in proportions:\n",
        "        # Calculate the number of rows for class 1 and class 0\n",
        "        num_rows_class_1 = int(sample_size * prop_1)\n",
        "        num_rows_class_0 = sample_size - num_rows_class_1  # Remaining for class 0\n",
        "\n",
        "        # Creating multiple sets will ensure that we are performing a thorough experiment across multiple sets\n",
        "        for _ in range(num_sets):\n",
        "            # Sample rows for class 1 and class 0\n",
        "            class_1_rows = df[df[target_column] == 1].sample(n=num_rows_class_1, random_state=random.randint(0, 1000))\n",
        "            class_0_rows = df[df[target_column] == 0].sample(n=num_rows_class_0, random_state=random.randint(0, 1000))\n",
        "\n",
        "            # Combine samples for each class to form a single set and shuffle to ensure that the class rows are not clustered\n",
        "            sampled_df = pd.concat([class_1_rows, class_0_rows]).sample(frac=1, random_state=random.randint(0, 1000)).reset_index(drop=True)\n",
        "            example_sets[f\"Set_{set_counter}_Prop_{prop_1}\"] = sampled_df\n",
        "            set_counter += 1\n",
        "\n",
        "    return example_sets\n",
        "\n"
      ],
      "metadata": {
        "id": "Mtv8bcNHaegg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_annoy_prompt = \"\"\"\n",
        "The following is a table with features in the first few columns and the label in the last column.\n",
        "I will supply multiple instances with features and the corresponding label for your reference.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8Izfs4K-qCjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def evaluate_in_context_performance(training_set, test_data, tokenizer, model, max_new_tokens=10, feature_description=feature_description, table_format=\"t-table\", show_template=True):\n",
        "    \"\"\"\n",
        "    Records the balance of the training set, and calculates accuracy, precision, recall, and F1 score for a set of in-context examples.\n",
        "    If training_set is None, performs zero-shot evaluation.\n",
        "\n",
        "    Parameters:\n",
        "    - training_set (pd.DataFrame or None): The training dataset or None for zero-shot.\n",
        "    - test_data (pd.DataFrame): The test dataset for evaluation.\n",
        "    - tokenizer: The tokenizer to process text inputs.\n",
        "    - model: The model for generating predictions.\n",
        "    - max_new_tokens (int): The maximum number of new tokens for generation.\n",
        "    - feature_description (pd.DataFrame or None): DataFrame with feature descriptions.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with balance of training set, accuracy, precision, recall, and f1 score.\n",
        "    \"\"\"\n",
        "    # Record Training Balance\n",
        "    description_prompt = generate_feature_label_descriptions(feature_description[feature_description[\"Feature\"].isin(test_data.columns.to_list())])\n",
        "    if table_format == \"t-table\":\n",
        "      if training_set is not None:\n",
        "          # Prepare in-context examples from the training set\n",
        "          in_context_data = \"\\n\".join([f\"|{'|'.join(map(str, row))}|\" for _, row in training_set.iterrows()])\n",
        "          # Display column names and data rows with prompt\n",
        "          in_context_examples = first_part + description_prompt + \"--- data ---\\n\" + \"|\" + \"|\".join(test_data.columns) + in_context_data\n",
        "      else:\n",
        "          in_context_examples = first_part + description_prompt + \"--- data ---\\n\" + \"|\" + \"|\".join(test_data.columns) + \"|\"\n",
        "    else:\n",
        "      in_context_examples = t_annoy_prompt\n",
        "      if training_set is not None:\n",
        "          in_context_data = \"\\n\".join([f\"|{'|'.join(map(str, row))}|\" for _, row in training_set.iterrows()])\n",
        "          in_context_examples += in_context_data\n",
        "    # Prepare Test Prompts and Labels\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    non_numeric_count = 0\n",
        "    start = show_template\n",
        "    for _, row in test_data.iterrows():\n",
        "        # Mask the label in the test row\n",
        "        row_values = row.tolist()\n",
        "        row_values[-1] = \"<MASK>\"  # Assuming the label is the last column\n",
        "        masked_test_row = f\"|{'|'.join(map(str, row_values))}|\"\n",
        "\n",
        "        # Combine in-context examples with the masked test row\n",
        "        full_prompt = in_context_examples + \"\\n\" + masked_test_row + \"\\n\"\n",
        "        full_prompt += \"\\nPlease use the supplied data to predict the <MASK> Approved_Flag.\\nAnswer: \" if table_format == \"t-table\" else \"\\nPlease use the supplied data to predict the <MASK> label.\\nAnswer: \"\n",
        "\n",
        "        answer = row[-1]  # Store the ground truth label\n",
        "        prompt_without_answer = full_prompt\n",
        "        print(\"Prompt template:\",prompt_without_answer)\n",
        "        print(\"Answer template:\",answer)\n",
        "        if start:\n",
        "          print(\"Prompt template:\",prompt_without_answer)\n",
        "          print(\"Answer template:\",answer)\n",
        "          start = False\n",
        "        # Tokenize and generate answer\n",
        "        inputs = tokenizer(prompt_without_answer, return_tensors=\"pt\").to(device)\n",
        "        if len(inputs) > 4096:\n",
        "            print(f\"Token size exceeded: {len(inputs)}\")\n",
        "            return\n",
        "        input_ids = inputs['input_ids']\n",
        "\n",
        "        # Generate predictions\n",
        "        outputs = model.generate(\n",
        "              input_ids=input_ids,\n",
        "              attention_mask=inputs['attention_mask'],\n",
        "              max_new_tokens=max_new_tokens\n",
        "        )\n",
        "        generated_answer = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "        # print(generated_answer) # if u wanna troubleshoot and see what predictions are generated\n",
        "        # Append true and predicted values\n",
        "        y_true.append(answer)\n",
        "        if len(generated_answer) > 0 and generated_answer[0].isdigit() and int(generated_answer[0]) in [0, 1]:\n",
        "          y_pred.append(int(generated_answer[0]))\n",
        "        else:\n",
        "          non_numeric_count += 1\n",
        "          if non_numeric_count > 15:\n",
        "            break\n",
        "          y_pred.append(\"invalid\")\n",
        "        del inputs, input_ids, outputs\n",
        "        # Debugging output for intermediate results\n",
        "        # print(\"Prompt without answer:\\n\", prompt_without_answer)\n",
        "        # print(\"Groundtruth:\", answer)\n",
        "        # print(\"Generated answer:\", generated_answer)\n",
        "        # print(\"---\")\n",
        "    print(\"Num of non-numeric or invalid:\", non_numeric_count)\n",
        "    # Calculate Metrics\n",
        "    pred = y_pred.copy()\n",
        "    filtered_y_true = [yt for yt, yp in zip(y_true, y_pred) if isinstance(yp, int)]\n",
        "    filtered_y_pred = [yp for yp in y_pred if isinstance(yp, int)]\n",
        "\n",
        "    # Convert back to arrays if needed\n",
        "    y_true = np.array(filtered_y_true)\n",
        "    y_pred = np.array(filtered_y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=\"warn\")\n",
        "    recall = recall_score(y_true, y_pred, zero_division=\"warn\")\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=\"warn\")\n",
        "\n",
        "    # Combine Results\n",
        "    results = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1\n",
        "    }\n",
        "\n",
        "    return results, pred\n"
      ],
      "metadata": {
        "id": "G6Eq-UHbfCm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top_features(feature_importance_df, top_n):\n",
        "    top_features = feature_importance_df.nlargest(top_n, 'Importance')['Feature'].tolist()\n",
        "    return top_features\n"
      ],
      "metadata": {
        "id": "p5VZdtg3kq3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def run_experiments(feature_importance_df, test_data, tokenizer, model, training_set_df, sample_size, proportions, num_sets=3, feature_set_sizes=[5, 10, 20, 30, 40], run_multiple=True, table_format=\"t-table\"):\n",
        "    \"\"\"\n",
        "    Runs experiments with varying feature sets, training set sizes with controlled imbalances, and records model performance and stability.\n",
        "\n",
        "    Parameters:\n",
        "    - feature_importance_df (pd.DataFrame): DataFrame with 'Feature' and 'Importance' columns.\n",
        "    - test_data (pd.DataFrame): Test dataset.\n",
        "    - tokenizer: Tokenizer for prompt processing.\n",
        "    - model: Model for generating predictions.\n",
        "    - training_set_df (pd.DataFrame): Original training dataset to sample in-context examples from.\n",
        "    - sample_size (int): Number of examples per set for the training data.\n",
        "    - proportions (list): List of proportions for class 1 in training sets to control imbalance.\n",
        "    - num_sets (int): Number of sets to generate per training set size and feature set size.\n",
        "    - feature_set_sizes (list): List of feature set sizes to evaluate.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A DataFrame recording results for each experiment and a dictionary storing in-context sets.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    stored_in_context_sets = {}\n",
        "\n",
        "    for top_n in feature_set_sizes:\n",
        "        # Select top features based on importance\n",
        "        top_features = select_top_features(feature_importance_df, top_n)\n",
        "        training_set_df_subset = training_set_df[top_features + ['Approved_Flag']]  # Subset training data\n",
        "        test_data_subset = test_data[top_features + ['Approved_Flag']]  # Subset test data with selected features\n",
        "        print(f\"\\nStarting experiments for top {top_n} features...\")\n",
        "\n",
        "        for prop_1 in proportions:\n",
        "            print(f\"\\nTesting with class 1 proportion: {prop_1}\")\n",
        "            # Generate sets of in-context examples for the current proportion of class 1\n",
        "            in_context_sets = create_in_context_example_sets(training_set_df_subset, sample_size, num_sets, 'Approved_Flag', [prop_1])\n",
        "\n",
        "            # Store each in-context set in the dictionary with a unique identifier\n",
        "            for set_id, in_context_examples in in_context_sets.items():\n",
        "                unique_id = f\"Features_{top_n}_Prop_{prop_1}_Set_{set_id}\"\n",
        "                stored_in_context_sets[unique_id] = in_context_examples  # Store the in-context set\n",
        "                print(f\"\\nEvaluating Set ID: {set_id}\")\n",
        "\n",
        "                y_pred_all = []  # To store predictions for stability check\n",
        "                times = 4 if run_multiple else 2\n",
        "                for run_num in range(1, times):  # 3 runs per set for stability assessment\n",
        "                    # Evaluate using in-context examples from `in_context_examples`\n",
        "                    results_dict, y_pred = evaluate_in_context_performance(in_context_examples, test_data_subset, tokenizer, model, table_format=table_format)\n",
        "\n",
        "                    # Record metrics\n",
        "                    run_results = {\n",
        "                        \"Num Features\": top_n,\n",
        "                        \"Sample Size\": sample_size,\n",
        "                        \"Class 1 Proportion\": prop_1,\n",
        "                        \"Set ID\": set_id,\n",
        "                        \"Run Number\": run_num,\n",
        "                        \"Accuracy\": results_dict[\"Accuracy\"],\n",
        "                        \"Precision\": results_dict[\"Precision\"],\n",
        "                        \"Recall\": results_dict[\"Recall\"],\n",
        "                        \"F1 Score\": results_dict[\"F1 Score\"],\n",
        "                        \"Prediction\": y_pred\n",
        "                    }\n",
        "                    results.append(run_results)\n",
        "\n",
        "                    # Print intermediate results for each run\n",
        "                    print(f\"Intermediate results for Run {run_num}: {run_results}\")\n",
        "\n",
        "    # Convert results to DataFrame for easy viewing and analysis\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\nAll experiments completed.\")\n",
        "    return results_df, stored_in_context_sets\n",
        "\n",
        "# Example usage:\n",
        "# proportions = [0.2, 0.5, 0.8]  # Different proportions of class 1 to create various imbalances\n",
        "# experiment_results, in_context_sets_dict = run_experiments(feature_importance_df, test_data, tokenizer, model, training_set_df, sample_size=50, proportions=proportions)\n",
        "# print(experiment_results)\n",
        "# print(\"Stored in-context sets:\", in_context_sets_dict.keys())\n"
      ],
      "metadata": {
        "id": "cnlji5itkrnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-shot (feature nums: 5, 10, 20, 30, 40) (ohe). Ignore this example and skip down to function 'save_in_context_sets_to_csv' and beyond"
      ],
      "metadata": {
        "id": "LPZYQPe7lwyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example"
      ],
      "metadata": {
        "id": "-gU70DfJMvcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# proportions = [0.1, 0.3, 0.5]  # Different proportions of class 1 to create various imbalances\n",
        "# experiment_results = run_experiments(feature_imp_ohe, test_ohe_100, tokenizer, model, train_ohe, sample_size=0, proportions=proportions)\n",
        "# print(experiment_results)"
      ],
      "metadata": {
        "id": "4EXhzrdUmcI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermediate results for Run 1: {'Num Features': 5, 'Sample Size': 0, 'Class 1 Proportion': 0.1, 'Set ID': 'Set_1_Prop_0.1', 'Run Number': 1, 'Accuracy': 0.69, 'Precision': 0.796547619047619, 'Recall': 0.69, 'F1 Score': 0.7320846755637368}\n",
        "\n",
        "\n",
        "Intermediate results for Run 2: {'Num Features': 5, 'Sample Size': 0, 'Class 1 Proportion': 0.1, 'Set ID': 'Set_1_Prop_0.1', 'Run Number': 2, 'Accuracy': 0.61, 'Precision': 0.7666577540106952, 'Recall': 0.61, 'F1 Score': 0.6703629536921152}\n",
        "\n",
        "Intermediate results for Run 3: {'Num Features': 5, 'Sample Size': 0, 'Class 1 Proportion': 0.1, 'Set ID': 'Set_1_Prop_0.1', 'Run Number': 3, 'Accuracy': 0.69, 'Precision': 0.796547619047619, 'Recall': 0.69, 'F1 Score': 0.7320846755637368}\n",
        "Instability count for Set Set_1_Prop_0.1 with proportion 0.1: 0"
      ],
      "metadata": {
        "id": "i2MgKXZD5Xwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def save_in_context_sets_to_csv(stored_in_context_sets, filename, output_dir=\"/content/drive/MyDrive/Indian_bank_data\"):\n",
        "    \"\"\"\n",
        "    Saves each in-context set in stored_in_context_sets as a separate CSV file, with each set having a unique column name.\n",
        "\n",
        "    Parameters:\n",
        "    - stored_in_context_sets (dict): Dictionary containing in-context sets with unique IDs as keys.\n",
        "    - filename (str): The base filename for the CSV files.\n",
        "    - output_dir (str): Directory where CSV files will be saved. Defaults to 'in_context_sets'.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Prepare a combined DataFrame to save all sets in a single CSV file\n",
        "    combined_df = pd.DataFrame()\n",
        "\n",
        "    # Iterate over each in-context set\n",
        "    for set_id, in_context_df in stored_in_context_sets.items():\n",
        "        # Add the set_id as a column name\n",
        "        in_context_df.columns = [f\"{set_id}_{col}\" for col in in_context_df.columns]\n",
        "\n",
        "        # Concatenate with combined_df\n",
        "        combined_df = pd.concat([combined_df, in_context_df], axis=1)\n",
        "\n",
        "        # List of columns to check and replace 'invalid'\n",
        "        metric_columns_to_replace_invalids = ['Accuracy', 'Precision', 'Recall', 'F1_Score', 'PR_AUC', 'ROC_AUC']\n",
        "        # Replace 'invalid' with 0 in the specified columns\n",
        "        combined_df[metric_columns_to_replace_invalids].replace('invalid', 0, inplace=True)\n",
        "\n",
        "    # Save the combined DataFrame as a single CSV with the specified filename\n",
        "    file_path = os.path.join(output_dir, f\"{filename}.csv\")\n",
        "    combined_df.to_csv(file_path, index=False)\n",
        "    print(f\"Saved all in-context sets to {file_path}\")\n",
        "\n",
        "# Example usage after running run_experiments:\n",
        "# save_in_context_sets_to_csv(in_context_sets_dict, filename=\"all_in_context_sets\")\n"
      ],
      "metadata": {
        "id": "xmxYAF9K9to4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example loop for t-table and t-annony"
      ],
      "metadata": {
        "id": "ixj-ghJBNGav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this. Run based on the...\n",
        "\n",
        "- sample_sizes\n",
        "- t-table/t-annony format\n",
        "\n",
        "that you need to run"
      ],
      "metadata": {
        "id": "9POMDvvGEobB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "proportions = [0.1, 0.3, 0.5]  # Different proportions of class 1 to create various imbalances\n",
        "sample_sizes = [0, 8, 16, 32, 48, 64]  # Different sample sizes to iterate over\n",
        "\n",
        "# Loop over each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    experiment_results, stored_in_context_sets = run_experiments(\n",
        "        feature_imp_ohe, test_ohe_100, tokenizer, model, train_ohe,\n",
        "        sample_size=sample_size, proportions=proportions,\n",
        "        feature_set_sizes=[5, 10, 20, 30, 40], num_sets=15,\n",
        "        run_multiple=False, table_format=\"t-table\"\n",
        "    )\n",
        "\n",
        "    # Save results to CSV\n",
        "    file_name = f\"experiments_result_t_table_show_{sample_size}_Nov21.csv\"\n",
        "    experiment_results.to_csv(file_name, index=False)\n",
        "\n",
        "    print(f\"Results for sample_size={sample_size} saved as {file_name}\")"
      ],
      "metadata": {
        "id": "rTZEhXgituJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "ab8f4970-cdd7-4eb0-b8cd-5c5dc876c2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-da7f33bae405>:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  answer = row[-1]  # Store the ground truth label\n",
            "<ipython-input-37-da7f33bae405>:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  answer = row[-1]  # Store the ground truth label\n",
            "<ipython-input-37-da7f33bae405>:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  answer = row[-1]  # Store the ground truth label\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-41a8d7c443cf>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loop over each sample size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     experiment_results, stored_in_context_sets = run_experiments(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mfeature_imp_ohe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ohe_100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ohe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproportions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproportions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-1bcafc551456>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(feature_importance_df, test_data, tokenizer, model, training_set_df, sample_size, proportions, num_sets, feature_set_sizes, run_multiple, table_format)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrun_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 3 runs per set for stability assessment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;31m# Evaluate using in-context examples from `in_context_examples`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mresults_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_in_context_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_context_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;31m# Record metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-da7f33bae405>\u001b[0m in \u001b[0;36mevaluate_in_context_performance\u001b[0;34m(training_set, test_data, tokenizer, model, max_new_tokens, feature_description, table_format, show_template)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     66\u001b[0m               \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m               \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_initial_cache_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3195\u001b[0;31m         while self._has_unfinished_sequences(\n\u001b[0m\u001b[1;32m   3196\u001b[0m             \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mthis_peer_finished_flag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2413\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2414\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "proportions = [0.1, 0.3, 0.5]  # Different proportions of class 1 to create various imbalances\n",
        "sample_sizes = [0, 8, 16, 32, 64, 128]  # Different sample sizes to iterate over\n",
        "\n",
        "# Loop over each sample size\n",
        "for sample_size in sample_sizes:\n",
        "    experiment_results, stored_in_context_sets = run_experiments(\n",
        "        feature_imp_ohe, test_ohe_100, tokenizer, model, train_ohe,\n",
        "        sample_size=sample_size, proportions=proportions,\n",
        "        feature_set_sizes=[5, 10, 20, 30, 40], num_sets=1,\n",
        "        run_multiple=False, table_format=\"t-annony\"\n",
        "    )\n",
        "\n",
        "    # Save results to CSV\n",
        "    file_name = f\"experiments_result_t_annoy_show_{sample_size}.csv\"\n",
        "    experiment_results.to_csv(file_name, index=False)\n",
        "\n",
        "    print(f\"Results for sample_size={sample_size} saved as {file_name}\")\n"
      ],
      "metadata": {
        "id": "ffnEp1uUu8q9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}